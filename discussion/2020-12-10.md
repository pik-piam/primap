# meeting 2020-12-10

Erfahrungen:
* openscm ändert sich noch ziemlich schnell, eher keine Basis für unseren eigenen Code.
* sehr sparse datensätze (mit vielen NaNs) sind in xarray unpraktisch (also, wenn mehr als 99% der Daten NaN sind), was bei hochdimensionellen Datensätzen wahrscheinlicher wird. Daher macht es mehr Sinn, mehrere datasets zu haben, z.B. einen pro source und nur zu kombinieren wenn nötig.
* bei pandas dataframes müssen metadaten händisch mitgeführt werden, also neue Funktionen für alle Kernfunktionen (addition etc.) gemacht werden. xarray bietet Unterstützung für attributes und skalara Dimensionen.

Plan ist: einen oder mehrere relativ niedrigdimensionelle Datensätze pro Source, abspeichern in xarray netcdf Dateien. Dann kann mit einem automatisierten Extraktor per datalad über alle Metadaten gesucht werden.

Nächste Schritte:
* Mika erstellt package mit Infrastruktur für Tests und Dokumentation (alles öffentlich, github / readthedocs etc.), dafür eigene Gruppe auf github, da viele Datenrepositories erwartet werden.
* Johannes testet, wie seine bisherige Arbeit auf das neue System überführt werden kann.
* Metadaten müssen definiert werden. Wir haben nicht viel Meinung, wie genau Dinge geschrieben werden, muss aber festgelegt und vereinheitlich werden. Für Dimensionen keine Überschneidung mit python reversed words (type, class, etc.) und keine Leerzeichen. Wir schauen alle nochmal über die Metadaten drüber, welche als Dimenionen in xarray umgesetzt werden sollten, welche als sekundäre coords und welche als attrs. Mika koordiniert das.
* Brauchen Dokumentation, wie man ein datalad Paket anlegt. datalad-Entwickler empfehlen, pro z.B. source ein datalad-Paket anzulegen und dann pro Analyse ein neues datalad-Paket mit Unter-Datensätzen. PIK-Infrastruktur hat noch keine Unterstüzung für datalad, braucht noch etwas Arbeit von Mika. Wir hätten dann auch ein großes datalad-Paket mit allen Datensätzen als Unter-Datensätze, so dass man komfortabel in allen Datensätzen auf einmal suchen kann.
* Für Metadaten sollten wir auch die wichtigsten Terminologien zusammensuchen und sauber definieren. Auf jeden Fall 2006er und 1996er IPCC Kategorien.

Datalad Zukunft:
* Datensätze erstmal intern auf dem pik gitlab + Datensätze auf den servern. Für veröffentlichte Datensätze kann dann das git-repo auf github.
* Wäre super, wenn es eine integration mit zenodo gäbe, so dass für veröffentlichte Datensätze user die Daten von dort kriegen können. Sonst gibt es immer das Problem, dass es keine Infrastruktur gibt, um große Datensätze öffentlich zugänglich zu machen.

